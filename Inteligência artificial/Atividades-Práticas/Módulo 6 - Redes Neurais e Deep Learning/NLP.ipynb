{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["AW8cH21xxGmk"],"gpuType":"T4","authorship_tag":"ABX9TyOo33EwTKM8WhN/Ekiu/Nq9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# NLP"],"metadata":{"id":"BJv247pUpoCC"}},{"cell_type":"markdown","source":["## IMDB"],"metadata":{"id":"AW8cH21xxGmk"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"fh-ONSYBmLe9","executionInfo":{"status":"ok","timestamp":1707326667765,"user_tz":180,"elapsed":3117,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"outputs":[],"source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","import keras\n","import tensorflow as tf\n","import numpy as np\n","from keras import layers"]},{"cell_type":"code","source":["!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CY4k3GamUy1","executionInfo":{"status":"ok","timestamp":1707326679530,"user_tz":180,"elapsed":11774,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"9ac7369a-a60c-415c-b251-f6fe0b2b1cf0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  51.0M      0  0:00:01  0:00:01 --:--:-- 51.0M\n"]}]},{"cell_type":"code","source":["!ls aclImdb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0soBdoNOmWtx","executionInfo":{"status":"ok","timestamp":1707326686856,"user_tz":180,"elapsed":6,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"1d8a0bd7-4988-4b47-a79e-eb515107d44d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"]}]},{"cell_type":"code","source":["!rm -r aclImdb/train/unsup"],"metadata":{"id":"93KqxjkonM-B","executionInfo":{"status":"ok","timestamp":1707326889238,"user_tz":180,"elapsed":2043,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","raw_train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=1337,\n",")\n","raw_val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=1337,\n",")\n","raw_test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","\n","print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n","print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n","print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Z3lLSCGmcH8","executionInfo":{"status":"ok","timestamp":1707326895248,"user_tz":180,"elapsed":4137,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"72fa3f6c-708f-4295-d28f-2ed47677443c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 2 classes.\n","Using 20000 files for training.\n","Found 25000 files belonging to 2 classes.\n","Using 5000 files for validation.\n","Found 25000 files belonging to 2 classes.\n","Number of batches in raw_train_ds: 625\n","Number of batches in raw_val_ds: 157\n","Number of batches in raw_test_ds: 782\n"]}]},{"cell_type":"code","source":["import string\n","import re\n","\n","def custom_standardization(input_data):\n","    lowercase = tf.strings.lower(input_data)\n","    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n","    return tf.strings.regex_replace(\n","        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n","    )\n","\n","# Model constants.\n","max_features = 20000\n","embedding_dim = 128\n","sequence_length = 500\n","\n","vectorize_layer = keras.layers.TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=max_features,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n"],"metadata":{"id":"8twnFutMm4Fp","executionInfo":{"status":"ok","timestamp":1707326995862,"user_tz":180,"elapsed":521,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Let's make a text-only dataset (no labels):\n","text_ds = raw_train_ds.map(lambda x, y: x)\n","# Let's call `adapt`:\n","vectorize_layer.adapt(text_ds)"],"metadata":{"id":"GecqBjV-nnfb","executionInfo":{"status":"ok","timestamp":1707327010713,"user_tz":180,"elapsed":5857,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def vectorize_text(text, label):\n","    text = tf.expand_dims(text, -1)\n","    return vectorize_layer(text), label\n","\n","\n","# Vectorize the data.\n","train_ds = raw_train_ds.map(vectorize_text)\n","val_ds = raw_val_ds.map(vectorize_text)\n","test_ds = raw_test_ds.map(vectorize_text)\n","\n","# Do async prefetching / buffering of the data for best performance on GPU.\n","train_ds = train_ds.cache().prefetch(buffer_size=10)\n","val_ds = val_ds.cache().prefetch(buffer_size=10)\n","test_ds = test_ds.cache().prefetch(buffer_size=10)"],"metadata":{"id":"iyqPqddOnx1h","executionInfo":{"status":"ok","timestamp":1707327072934,"user_tz":180,"elapsed":993,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# A integer input for vocab indices.\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","\n","# Next, we add a layer to map those vocab indices into a space of dimensionality\n","# 'embedding_dim'.\n","x = layers.Embedding(max_features, embedding_dim)(inputs)\n","x = layers.Dropout(0.5)(x)\n","\n","# Conv1D + global max pooling\n","x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n","x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","\n","# We add a vanilla hidden layer:\n","x = layers.Dense(128, activation=\"relu\")(x)\n","x = layers.Dropout(0.5)(x)\n","\n","# We project onto a single unit output layer, and squash it with a sigmoid:\n","predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n","\n","model = keras.Model(inputs, predictions)\n","\n","# Compile the model with binary crossentropy loss and an adam optimizer.\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"xwbsJOF2n6M2","executionInfo":{"status":"ok","timestamp":1707327108755,"user_tz":180,"elapsed":2,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["epochs = 3\n","\n","# Fit the model using the train and test datasets.\n","model.fit(train_ds, validation_data=val_ds, epochs=epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFwFSTUfoC02","executionInfo":{"status":"ok","timestamp":1707327202243,"user_tz":180,"elapsed":92310,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"609c463a-e2ef-468f-c9a7-38fb03a944e7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","625/625 [==============================] - 51s 71ms/step - loss: 0.4938 - accuracy: 0.7200 - val_loss: 0.3153 - val_accuracy: 0.8652\n","Epoch 2/3\n","625/625 [==============================] - 5s 8ms/step - loss: 0.2230 - accuracy: 0.9133 - val_loss: 0.3240 - val_accuracy: 0.8728\n","Epoch 3/3\n","625/625 [==============================] - 4s 6ms/step - loss: 0.1183 - accuracy: 0.9553 - val_loss: 0.3871 - val_accuracy: 0.8716\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x78966cdd7160>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["model.evaluate(test_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qxMmZrEoDW4","executionInfo":{"status":"ok","timestamp":1707327254350,"user_tz":180,"elapsed":10749,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"d1666ad7-23d7-4fc5-b8ea-00864fa6102a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 5s 7ms/step - loss: 0.4006 - accuracy: 0.8600\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.40057802200317383, 0.8599600195884705]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["inputs = keras.Input(shape=(1,), dtype=\"string\")\n","# Turn strings into vocab indices\n","indices = vectorize_layer(inputs)\n","# Turn vocab indices into predictions\n","outputs = model(indices)\n","\n","# Our end to end model\n","end_to_end_model = keras.Model(inputs, outputs)\n","end_to_end_model.compile(\n","    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"_QpzPr-gokIb","executionInfo":{"status":"ok","timestamp":1707327265691,"user_tz":180,"elapsed":467,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["new_texts = [\"This movie is the worst\", \"This movie is so nice :)\", \"good and bad\"]\n","predictions = end_to_end_model.predict(np.array(new_texts))\n","\n","# Exibindo as previs√µes\n","for i, text in enumerate(new_texts):\n","    print(f\"Texto: {text}, Previs√£o: {predictions[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuWxolXYopej","executionInfo":{"status":"ok","timestamp":1707327492766,"user_tz":180,"elapsed":487,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"981cdf69-68a8-4974-b21d-44d4988a6e84"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 126ms/step\n","Texto: This movie is the worst, Previs√£o: [0.00719088]\n","Texto: This movie is so nice :), Previs√£o: [0.84028924]\n","Texto: good and bad, Previs√£o: [0.49212793]\n"]}]},{"cell_type":"markdown","source":["## Generating Text"],"metadata":{"id":"xrIhjKsAqmk3"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time"],"metadata":{"id":"IweKoCDZxoBJ","executionInfo":{"status":"ok","timestamp":1707416035441,"user_tz":180,"elapsed":3285,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oUTezJoxr44","executionInfo":{"status":"ok","timestamp":1707416036455,"user_tz":180,"elapsed":1016,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"807851a6-cf5b-4583-c820-e6f1d3cb6e8e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# Read, then decode for py2 compat.\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# length of text is the number of characters in it\n","print(f'Length of text: {len(text)} characters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNnzRcJCxwk1","executionInfo":{"status":"ok","timestamp":1707416036456,"user_tz":180,"elapsed":7,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"cef9a427-2d5c-4b91-cfc2-9778659c6d79"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 1115394 characters\n"]}]},{"cell_type":"code","source":["# Take a look at the first 250 characters in text\n","print(text[:250])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTNB3C7Zx0bv","executionInfo":{"status":"ok","timestamp":1707416036456,"user_tz":180,"elapsed":5,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"851a957b-b9c6-4bb6-d5da-4b3a3407a394"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"]}]},{"cell_type":"code","source":["# The unique characters in the file\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gabk2D0Ax8dx","executionInfo":{"status":"ok","timestamp":1707416036456,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"f3a2851f-4087-4e9d-cfa6-acf44e84c07e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n"]}]},{"cell_type":"code","source":["ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), mask_token=None)"],"metadata":{"id":"D4L2F1y2ybmy","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":918,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"],"metadata":{"id":"B5hPmutWypUr","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":6,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","all_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btJB2N0vyR0R","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":5,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"c35f913c-ada3-45ca-a055-b0f1f43cc300"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n","for ids in ids_dataset.take(10):\n","    print(chars_from_ids(ids).numpy().decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xch-DwKAyWWH","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":5,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"527f06f9-e362-4055-e40d-6a343c6debe1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["F\n","i\n","r\n","s\n","t\n"," \n","C\n","i\n","t\n","i\n"]}]},{"cell_type":"code","source":["seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"],"metadata":{"id":"WjohpAxqy6l6","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","split_input_target(list(\"Tensorflow\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MI_-Gt3VzG0z","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"669bc3b2-131c-43dc-d416-2421a7e61c07"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n"," ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["dataset = sequences.map(split_input_target)"],"metadata":{"id":"wOnWU4vBzZaa","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xvuo-axOzVXO","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"aed0751e-7312-4c88-9303-98a963e509a9"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"metadata":{"id":"jva-Y_nIz0YF","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class MyModel(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x"],"metadata":{"id":"IDUr8t3xz5dn","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = MyModel(\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)"],"metadata":{"id":"lUFSxXQ-01FY","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"g6HKSl4506MM","executionInfo":{"status":"ok","timestamp":1707416037371,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sieeKxrY1NBL","executionInfo":{"status":"ok","timestamp":1707416042153,"user_tz":180,"elapsed":4785,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"2c931617-8549-4192-c5ae-3e69d3909c3c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"]}]},{"cell_type":"code","source":["example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"Mean loss:        \", example_batch_mean_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BU82MEap1HcY","executionInfo":{"status":"ok","timestamp":1707416042154,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"487494d4-874d-4c61-956a-eb1154eaf1ee"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n","Mean loss:         tf.Tensor(4.189551, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss=loss)"],"metadata":{"id":"K4ygsTCe1T3A","executionInfo":{"status":"ok","timestamp":1707416042154,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"metadata":{"id":"-11rVs2T1XVG","executionInfo":{"status":"ok","timestamp":1707416042154,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 20\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6bxwfk51XOC","executionInfo":{"status":"ok","timestamp":1707416327691,"user_tz":180,"elapsed":285540,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"5d4e0257-edf2-40d5-f093-9c8ade7414d1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","172/172 [==============================] - 21s 69ms/step - loss: 2.7130\n","Epoch 2/20\n","172/172 [==============================] - 14s 53ms/step - loss: 1.9839\n","Epoch 3/20\n","172/172 [==============================] - 11s 53ms/step - loss: 1.7066\n","Epoch 4/20\n","172/172 [==============================] - 11s 54ms/step - loss: 1.5466\n","Epoch 5/20\n","172/172 [==============================] - 11s 54ms/step - loss: 1.4485\n","Epoch 6/20\n","172/172 [==============================] - 11s 55ms/step - loss: 1.3808\n","Epoch 7/20\n","172/172 [==============================] - 12s 54ms/step - loss: 1.3289\n","Epoch 8/20\n","172/172 [==============================] - 12s 55ms/step - loss: 1.2847\n","Epoch 9/20\n","172/172 [==============================] - 12s 54ms/step - loss: 1.2433\n","Epoch 10/20\n","172/172 [==============================] - 12s 55ms/step - loss: 1.2030\n","Epoch 11/20\n","172/172 [==============================] - 12s 55ms/step - loss: 1.1641\n","Epoch 12/20\n","172/172 [==============================] - 12s 56ms/step - loss: 1.1219\n","Epoch 13/20\n","172/172 [==============================] - 12s 56ms/step - loss: 1.0796\n","Epoch 14/20\n","172/172 [==============================] - 11s 55ms/step - loss: 1.0342\n","Epoch 15/20\n","172/172 [==============================] - 11s 56ms/step - loss: 0.9842\n","Epoch 16/20\n","172/172 [==============================] - 11s 56ms/step - loss: 0.9334\n","Epoch 17/20\n","172/172 [==============================] - 11s 56ms/step - loss: 0.8817\n","Epoch 18/20\n","172/172 [==============================] - 11s 56ms/step - loss: 0.8291\n","Epoch 19/20\n","172/172 [==============================] - 12s 57ms/step - loss: 0.7785\n","Epoch 20/20\n","172/172 [==============================] - 12s 57ms/step - loss: 0.7271\n"]}]},{"cell_type":"code","source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","\n","    # Return the characters and model state.\n","    return predicted_chars, states"],"metadata":{"id":"bSlXfJ7b2QWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"metadata":{"id":"Vt7ElXeH2S1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['ROMEO:'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"id":"ZFS534Tf2XCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BERT"],"metadata":{"id":"dg5WjqNOqAuE"}},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Carregar o modelo BERT pr√©-treinado para classifica√ß√£o de sequ√™ncias\n","model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","\n","# Texto de exemplo para classifica√ß√£o\n","text = \"bad and good things\"\n","\n","# Tokeniza√ß√£o do texto\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","# Passando o texto tokenizado para o modelo para classifica√ß√£o\n","outputs = model(**inputs)\n","\n","# Obtendo as previs√µes de classe\n","predictions = torch.softmax(outputs.logits, dim=1)\n","\n","# Obter a classe prevista\n","predicted_class = torch.argmax(predictions, dim=1).item()\n","\n","# Mapeamento de classes\n","class_mapping = {0: \"negativo\", 1: \"positivo\"}\n","\n","# Exibindo a previs√£o e a classe prevista\n","print(\"Previs√µes de classe:\", predictions)\n","print(\"Classe prevista:\", class_mapping[predicted_class])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xK-RTa2-pKN","executionInfo":{"status":"ok","timestamp":1707417327134,"user_tz":180,"elapsed":2567,"user":{"displayName":"Matheus Bento","userId":"09617022459127220926"}},"outputId":"1f14cab3-d62d-4c16-b7d6-218825c2768f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Previs√µes de classe: tensor([[0.6222, 0.3778]], grad_fn=<SoftmaxBackward0>)\n","Classe prevista: negativo\n"]}]}]}